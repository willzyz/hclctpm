{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, cPickle as pkl \n",
    "import sys, os \n",
    "sys.path.append('../dataprep/') \n",
    "from QueryFunctions import * \n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlabel_dates_weekly = \"\\'2019-10-06\\', \\'2019-10-13\\', \\'2019-10-20\\', \\'2019-10-27\\'\" \\ncity_ids = \\'1,5,6,8,10,12,20,23,198\\' \\nfeature_dates=\"\\'2019-09-22\\'\" \\nproposal_start_date = \\'2019-09-30\\' \\n\\npredFrame4 = qr.execute(\\'presto\\', rxgy_data_sapphire_presto_featuremod2(label_dates_weekly, feature_dates,  city_ids)) \\npredFrame4 = pd.DataFrame(predFrame4.load_data()) \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_query = 1 ### turn this switch on (to 1.0/True vs 0.0/False) to run query using QueryRunner \n",
    "\n",
    "if use_query: \n",
    "    from queryrunner_client import Client \n",
    "    qr = Client(user_email='will.zou@uber.com') \n",
    "    label_dates_weekly = \"'2019-07-14', '2019-07-21', '2019-07-28', '2019-08-04'\" \n",
    "    city_ids = '1,5,6,8,10,12,20,23,198' \n",
    "    feature_dates=\"'2019-06-30'\" \n",
    "    proposal_start_date = '2019-07-08' \n",
    "\n",
    "    predFrame2 = qr.execute('presto', rxgy_data_sapphire_presto_featuremod2(label_dates_weekly, feature_dates,  city_ids)) \n",
    "    predFrame2 = pd.DataFrame(predFrame2.load_data()) \n",
    "else: \n",
    "    predFrame2 = pd.read_csv('../data/rxgy_query_result.csv') \n",
    "\n",
    "\"\"\" \n",
    "label_dates_weekly = \"'2019-07-07','2019-06-30','2019-06-23','2019-06-16'\"\n",
    "city_ids = '1,5,6,8,10,12,20,23,198' \n",
    "feature_dates=\"'2019-06-02'\" \n",
    "proposal_start_date = '2019-06-10' \n",
    "\n",
    "predFrame = qr.execute('presto', rxgy_data_sapphire_presto_featuremod2(label_dates_weekly, feature_dates,  city_ids)) \n",
    "predFrame = pd.DataFrame(predFrame.load_data()) \n",
    "\"\"\" \n",
    "\n",
    "\"\"\" \n",
    "label_dates_weekly = \"'2019-08-11', '2019-08-18', '2019-08-25', '2019-09-01'\" \n",
    "city_ids = '1,5,6,8,10,12,20,23,198' \n",
    "feature_dates=\"'2019-07-28'\" \n",
    "proposal_start_date = '2019-08-05' \n",
    "\n",
    "predFrame3 = qr.execute('presto', rxgy_data_sapphire_presto_featuremod2(label_dates_weekly, feature_dates,  city_ids)) \n",
    "predFrame3 = pd.DataFrame(predFrame3.load_data()) \n",
    "\"\"\" \n",
    "\n",
    "\"\"\"\n",
    "label_dates_weekly = \"'2019-10-06', '2019-10-13', '2019-10-20', '2019-10-27'\" \n",
    "city_ids = '1,5,6,8,10,12,20,23,198' \n",
    "feature_dates=\"'2019-09-22'\" \n",
    "proposal_start_date = '2019-09-30' \n",
    "\n",
    "predFrame4 = qr.execute('presto', rxgy_data_sapphire_presto_featuremod2(label_dates_weekly, feature_dates,  city_ids)) \n",
    "predFrame4 = pd.DataFrame(predFrame4.load_data()) \n",
    "\"\"\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "964520\n",
      "192911\n"
     ]
    }
   ],
   "source": [
    "cohort_column_name = 'cohort' \n",
    "treatment_indicator_value = 'treatment' \n",
    "control_indicator_value = 'control' \n",
    "\n",
    "print(len(predFrame2))\n",
    "print(sum(predFrame2[cohort_column_name] == control_indicator_value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/pandas/core/ops.py:1649: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n",
      "number of nans: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rpu_treated : 4.04222604972\n",
      "nipu_treated : -1.10895217284\n",
      "rpu_untreated : 3.40695968607\n",
      "nipu_untreated : 7.75802695467\n",
      "cpit : 13.9578917363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:89: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:91: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:93: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:95: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "### preprocess the data \n",
    "### -- sample treatment to match control cohort \n",
    "### -- eliminate nulls, standard normalization \n",
    "\n",
    "D = predFrame2 \n",
    "D = D.sample(frac=1.0) \n",
    "\n",
    "feature_list = [ \n",
    "    'rating_2driver_min_avg_84d'\n",
    "    , 'trip_incomplete_total_84d'\n",
    "    , 'days_active_84d'\n",
    "    , 'days_since_trip_first_lifetime'\n",
    "    , 'days_since_last_hard_churn_lifetime'\n",
    "    , 'days_since_last_soft_churn_lifetime'\n",
    "    , 'fare_max_sd_84d'\n",
    "    , 'churns_hard_lifetime'\n",
    "    , 'trips_lifetime'\n",
    "    , 'fare_max_p50_84d'\n",
    "    , 'duration_session_pre_request_max_p50_84d'\n",
    "    , 'trip_pool_per_x_84d'\n",
    "    , 'fare_total_win7d_sd_84d'\n",
    "    , 'trip_complete_win7d_sd_84d'\n",
    "    , 'session_per_days_active_84d'\n",
    "    , 'churns_soft_lifetime'\n",
    "    , 'trip_complete_per_days_active_84d'\n",
    "    , 'trip_pool_prc_84d'\n",
    "    , 'session_background_pre_request_prc_84d'\n",
    "    , 'session_lt_1m_prc_84d'\n",
    "    , 'session_request_prc_84d'\n",
    "    , 'duration_session_outside_total_prc_84d'\n",
    "    , 'trip_x_prc_84d'\n",
    "    , 'days_since_trip_last_lifetime'\n",
    "    , 'promo_used_84d'\n",
    "    , 'has_session_request_84d'\n",
    "    , 'has_session_without_request_84d' \n",
    "    , 'fare_promo_total_avg_84d', \n",
    "    'fare_total_avg_84d', \n",
    "    'surge_trip_avg_84d', \n",
    "    'fare_total_win7d_potential_84d', \n",
    "    'fare_total_win28d_potential_84d', \n",
    "    'fare_lifetime', \n",
    "    'time_to_first_message_minutes_mean_lifetime', \n",
    "    'ata_trip_max_avg_84d', \n",
    "    'eta_trip_max_avg_84d', \n",
    "    'trip_pool_matched_avg_84d', \n",
    "    'payment_cash_trip_total_84d', \n",
    "    'duration_trip_total_p50_84d'\n",
    "] \n",
    "\n",
    "label_list = [ \n",
    "    'label_trip_28d', \n",
    "    'label_vc_28d' \n",
    "] \n",
    "\n",
    "for l in feature_list: \n",
    "    print('number of nans: ' + str(sum(D[l] == '\\N'))) \n",
    "    D[l] = pd.to_numeric(D[l], errors='coerce') \n",
    "    D[l] = D[l] - D[l].mean() \n",
    "    D[l] = D[l] / D[l].std() \n",
    "    D[l][pd.isnull(D[l])] = 0.0 ## at zero mean due to standard normalization \n",
    "\n",
    "for l in label_list: \n",
    "    D[l] = pd.to_numeric(D[l], errors='coerce') \n",
    "    D[l][pd.isnull(D[l])] = 0.0 ## at zero mean due to standard normalization \n",
    "\n",
    "### -- compute simple statistics \n",
    "### compute cpit \n",
    "treated_entries = D[D[cohort_column_name] == treatment_indicator_value] \n",
    "untreated_entries = D[D[cohort_column_name] == control_indicator_value] \n",
    "\n",
    "rpu_treated = float(treated_entries[label_list[0]].sum()) / len(treated_entries) \n",
    "nipu_treated = float(treated_entries[label_list[1]].sum()) / len(treated_entries) \n",
    "\n",
    "rpu_untreated = float(untreated_entries[label_list[0]].sum()) / len(untreated_entries) \n",
    "nipu_untreated = float(untreated_entries[label_list[1]].sum()) / len(untreated_entries) \n",
    "\n",
    "cpit = -1.0 * (nipu_treated - nipu_untreated) / (rpu_treated - rpu_untreated) \n",
    "\n",
    "print('rpu_treated : ' + str(rpu_treated)) \n",
    "print('nipu_treated : ' + str(nipu_treated)) \n",
    "print('rpu_untreated : ' + str(rpu_untreated)) \n",
    "print('nipu_untreated : ' + str(nipu_untreated)) \n",
    "print('cpit : ' + str(cpit)) \n",
    "\n",
    "### split the data into 3/1/1 train/val/test \n",
    "len_tr = len(D) / 5 * 3 \n",
    "len_va = len(D) / 5 \n",
    "\n",
    "nX = D[feature_list].as_matrix() \n",
    "w = D[cohort_column_name].apply(lambda x: 1.0 if x == treatment_indicator_value else 0.0) \n",
    "w = w.as_matrix() \n",
    "values = D[label_list[0]] \n",
    "values = values.as_matrix() \n",
    "negcost = D[label_list[1]] \n",
    "negcost = negcost.as_matrix() * 1.0 \n",
    "\n",
    "## split train/val/test sets \n",
    "\n",
    "nX_tr = nX[0:len_tr, :] \n",
    "nX_va = nX[len_tr:len_tr + len_va, :] \n",
    "nX_te = nX[len_tr + len_va:, :] \n",
    "\n",
    "w_tr = w[0:len_tr]\n",
    "w_va = w[len_tr:len_tr + len_va] \n",
    "w_te = w[len_tr + len_va:] \n",
    "\n",
    "values_tr = values[0:len_tr] \n",
    "values_va = values[len_tr:len_tr + len_va] \n",
    "values_te = values[len_tr + len_va:] \n",
    "\n",
    "negcost_tr = negcost[0:len_tr] \n",
    "\n",
    "negcost_va = negcost[len_tr:len_tr + len_va] \n",
    "\n",
    "negcost_te = negcost[len_tr + len_va:] \n",
    "\n",
    "## saving data using cPickel and naming the dictionaries \n",
    "saveD = {'nX_tr':nX_tr, \n",
    "         'w_tr':w_tr, \n",
    "         'values_tr':values_tr, \n",
    "         'nX_va':nX_va, \n",
    "         'w_va':w_va, \n",
    "         'values_va':values_va, \n",
    "         'nX_te':nX_te, \n",
    "         'w_te':w_te, \n",
    "         'values_te':values_te, \n",
    "         'feature_list':feature_list, \n",
    "         #'avg_ni_usd_tr':avg_ni_usd_tr, \n",
    "         'negcost_tr': negcost_tr, \n",
    "         #'avg_ni_usd_va':avg_ni_usd_va, \n",
    "         'negcost_va': negcost_va, \n",
    "         #'avg_ni_usd_te':avg_ni_usd_te, \n",
    "         'negcost_te': negcost_te \n",
    "         } \n",
    "\n",
    "pkl.dump(saveD, open('../data/rxgy_ma_training_data_v5_2019_07_08_vc_tr_featuremod3', 'w')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data from ../data/rxgy_ma_training_data_v5_2019_07_08_vc_tr_featuremod3\n",
      "printing averages of c_tr, c_unt, o_tre, o_unt ... :\n",
      "1.111290521074603\n",
      "-7.809207073332213\n",
      "4.037701323251418\n",
      "3.400191648609684\n",
      "### ----- start the training of deep learning models ------ \n",
      "------> Training TQR ranking model .... \n",
      "---> running cross validation, iteration: 0\n",
      "WARNING:tensorflow:From ../ModelDefinitions.py:37: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From ../ModelDefinitions.py:37: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "WARNING:tensorflow:From ../ModelDefinitions.py:38: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a99f4ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a99f4ad0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a99f4ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a99f4ad0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a99f4bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a99f4bd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a99f4bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a99f4bd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From ../ModelDefinitions.py:74: The name tf.ceil is deprecated. Please use tf.math.ceil instead.\n",
      "\n",
      "WARNING:tensorflow:From ../ModelDefinitions.py:115: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "opt. step : 0 obj: 14.911174033114916\n",
      "setting temperature to :0.6\n",
      "---> optimization finished ... \n",
      "temp:\n",
      "0.5278938128198591\n",
      "p_quantile:\n",
      "0.4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1b498cc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1b498cc50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1b498cc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1b498cc50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1b47f6bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1b47f6bd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1b47f6bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1b47f6bd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "validation CPIT:\n",
      "14.23151470592113\n",
      "---> running cross validation, iteration: 1\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1b4a5f490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1b4a5f490>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1b4a5f490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1b4a5f490>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1b4a5f910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1b4a5f910>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1b4a5f910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1b4a5f910>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "opt. step : 0 obj: 13.55305571239784\n",
      "setting temperature to :0.6\n",
      "---> optimization finished ... \n",
      "temp:\n",
      "0.7036850396746279\n",
      "p_quantile:\n",
      "0.4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1c78b66d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1c78b66d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1c78b66d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1c78b66d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1c78b6650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1c78b6650>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1c78b6650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1c78b6650>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "validation CPIT:\n",
      "13.064114150904961\n",
      "best performing model: iteration 1\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d8eb8c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d8eb8c10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d8eb8c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d8eb8c10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "------> Training DRM ranking model .... \n",
      "---> running cross validation, iteration: 0\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d8d60150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d8d60150>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d8d60150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d8d60150>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d8e72d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d8e72d50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d8e72d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d8e72d50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "opt. step : 0 obj: 14.153775123663017\n",
      "---> optimization finished ... \n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1db1ae910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1db1ae910>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1db1ae910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1db1ae910>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1db1ae510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1db1ae510>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1db1ae510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1db1ae510>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "validation CPIT:\n",
      "12.192187739950203\n",
      "---> running cross validation, iteration: 1\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d04d38d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d04d38d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d04d38d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d04d38d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d04d3f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d04d3f50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d04d3f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d04d3f50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "opt. step : 0 obj: 14.673885038445786\n",
      "---> optimization finished ... \n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d3ff6910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d3ff6910>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d3ff6910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d3ff6910>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d40cd3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d40cd3d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d40cd3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d40cd3d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "validation CPIT:\n",
      "12.255395142793034\n",
      "best performing model: iteration 0\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d41a3f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d41a3f90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d41a3f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1d41a3f90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "rpu_control: 3.4214800975660387\n",
      "nipu_control: 7.671282771491557\n",
      "rpu_ft: 4.0606804607232165\n",
      "nipu_ft: -1.02621382073425\n",
      "---------------------------->>>>>>\n",
      "perc - target: 0.10\n",
      "treated_target_rpu: 4.03\n",
      "treated_target_nipu: -1.04\n",
      "nontreated_target_rpu: 3.57\n",
      "nontreated_target_nipu: 7.41\n",
      "treated_nontarget_rpu: 4.06\n",
      "treated_nontarget_nipu: -1.02\n",
      "nontreated_nontarget_rpu: 3.40\n",
      "nontreated_nontarget_nipu: 7.70\n",
      "--- with 9.99979264297267% targeting, print cpits to treat users and create incrementality in users ---\n",
      "--> in targeted users: \n",
      "cpit = 18.484369872445225\n",
      "--> in non-targeted users: \n",
      "cpit = 13.225658907753324\n",
      "rpu_control: 3.42\n",
      "nipu_control: 7.67\n",
      "rpu_ft: 4.06\n",
      "nipu_ft: -1.03\n",
      "rpu_cohort: 3.47\n",
      "nipu_cohort: 6.83\n",
      "lift targeted cohort vs control: 0.01\n",
      "lift random vs control: 0.19\n",
      "cpit cohort vs control: 18.60\n",
      "lift targeted-treated vs control: 0.18\n",
      "cpit cohort: 18.596329\n",
      "---------------------------->>>>>>\n",
      "perc - target: 0.20\n",
      "treated_target_rpu: 4.07\n",
      "treated_target_nipu: -1.08\n",
      "nontreated_target_rpu: 3.51\n",
      "nontreated_target_nipu: 7.66\n",
      "treated_nontarget_rpu: 4.06\n",
      "treated_nontarget_nipu: -1.01\n",
      "nontreated_nontarget_rpu: 3.40\n",
      "nontreated_nontarget_nipu: 7.67\n",
      "--- with 19.99958528594534% targeting, print cpits to treat users and create incrementality in users ---\n",
      "--> in targeted users: \n",
      "cpit = 15.606589534607691\n",
      "--> in non-targeted users: \n",
      "cpit = 13.176015400573199\n",
      "rpu_control: 3.42\n",
      "nipu_control: 7.67\n",
      "rpu_ft: 4.06\n",
      "nipu_ft: -1.03\n",
      "rpu_cohort: 3.53\n",
      "nipu_cohort: 5.92\n",
      "lift targeted cohort vs control: 0.03\n",
      "lift random vs control: 0.19\n",
      "cpit cohort vs control: 15.64\n",
      "lift targeted-treated vs control: 0.19\n",
      "cpit cohort: 15.638871\n",
      "---------------------------->>>>>>\n",
      "perc - target: 0.30\n",
      "treated_target_rpu: 4.04\n",
      "treated_target_nipu: -1.06\n",
      "nontreated_target_rpu: 3.49\n",
      "nontreated_target_nipu: 7.74\n",
      "treated_nontarget_rpu: 4.07\n",
      "treated_nontarget_nipu: -1.01\n",
      "nontreated_nontarget_rpu: 3.39\n",
      "nontreated_nontarget_nipu: 7.64\n",
      "--- with 29.99937792891801% targeting, print cpits to treat users and create incrementality in users ---\n",
      "--> in targeted users: \n",
      "cpit = 15.913414472638282\n",
      "--> in non-targeted users: \n",
      "cpit = 12.803443885457135\n",
      "rpu_control: 3.42\n",
      "nipu_control: 7.67\n",
      "rpu_ft: 4.06\n",
      "nipu_ft: -1.03\n",
      "rpu_cohort: 3.59\n",
      "nipu_cohort: 5.03\n",
      "lift targeted cohort vs control: 0.05\n",
      "lift random vs control: 0.19\n",
      "cpit cohort vs control: 15.90\n",
      "lift targeted-treated vs control: 0.18\n",
      "cpit cohort: 15.896396\n",
      "---------------------------->>>>>>\n",
      "perc - target: 0.40\n",
      "treated_target_rpu: 4.08\n",
      "treated_target_nipu: -1.04\n",
      "nontreated_target_rpu: 3.47\n",
      "nontreated_target_nipu: 7.68\n",
      "treated_nontarget_rpu: 4.05\n",
      "treated_nontarget_nipu: -1.02\n",
      "nontreated_nontarget_rpu: 3.39\n",
      "nontreated_nontarget_nipu: 7.67\n",
      "--- with 39.99917057189068% targeting, print cpits to treat users and create incrementality in users ---\n",
      "--> in targeted users: \n",
      "cpit = 14.213224723022105\n",
      "--> in non-targeted users: \n",
      "cpit = 13.23717248619959\n",
      "rpu_control: 3.42\n",
      "nipu_control: 7.67\n",
      "rpu_ft: 4.06\n",
      "nipu_ft: -1.03\n",
      "rpu_cohort: 3.67\n",
      "nipu_cohort: 4.19\n",
      "lift targeted cohort vs control: 0.07\n",
      "lift random vs control: 0.19\n",
      "cpit cohort vs control: 14.20\n",
      "lift targeted-treated vs control: 0.19\n",
      "cpit cohort: 14.201071\n",
      "---------------------------->>>>>>\n",
      "perc - target: 0.50\n",
      "treated_target_rpu: 4.06\n",
      "treated_target_nipu: -1.06\n",
      "nontreated_target_rpu: 3.45\n",
      "nontreated_target_nipu: 7.61\n",
      "treated_nontarget_rpu: 4.06\n",
      "treated_nontarget_nipu: -1.00\n",
      "nontreated_nontarget_rpu: 3.39\n",
      "nontreated_nontarget_nipu: 7.73\n",
      "--- with 49.99896321486335% targeting, print cpits to treat users and create incrementality in users ---\n",
      "--> in targeted users: \n",
      "cpit = 14.091857587649724\n",
      "--> in non-targeted users: \n",
      "cpit = 13.155344608923354\n",
      "rpu_control: 3.42\n",
      "nipu_control: 7.67\n",
      "rpu_ft: 4.06\n",
      "nipu_ft: -1.03\n",
      "rpu_cohort: 3.73\n",
      "nipu_cohort: 3.34\n",
      "lift targeted cohort vs control: 0.09\n",
      "lift random vs control: 0.19\n",
      "cpit cohort vs control: 14.09\n",
      "lift targeted-treated vs control: 0.19\n",
      "cpit cohort: 14.094177\n",
      "---------------------------->>>>>>\n",
      "perc - target: 0.60\n",
      "treated_target_rpu: 4.06\n",
      "treated_target_nipu: -0.98\n",
      "nontreated_target_rpu: 3.43\n",
      "nontreated_target_nipu: 7.51\n",
      "treated_nontarget_rpu: 4.07\n",
      "treated_nontarget_nipu: -1.09\n",
      "nontreated_nontarget_rpu: 3.41\n",
      "nontreated_nontarget_nipu: 7.91\n",
      "--- with 59.99875585783602% targeting, print cpits to treat users and create incrementality in users ---\n",
      "--> in targeted users: \n",
      "cpit = 13.473577698639158\n",
      "--> in non-targeted users: \n",
      "cpit = 13.802199789185828\n",
      "rpu_control: 3.42\n",
      "nipu_control: 7.67\n",
      "rpu_ft: 4.06\n",
      "nipu_ft: -1.03\n",
      "rpu_cohort: 3.80\n",
      "nipu_cohort: 2.57\n",
      "lift targeted cohort vs control: 0.11\n",
      "lift random vs control: 0.19\n",
      "cpit cohort vs control: 13.47\n",
      "lift targeted-treated vs control: 0.19\n",
      "cpit cohort: 13.472207\n",
      "---------------------------->>>>>>\n",
      "perc - target: 0.70\n",
      "treated_target_rpu: 4.05\n",
      "treated_target_nipu: -0.99\n",
      "nontreated_target_rpu: 3.42\n",
      "nontreated_target_nipu: 7.57\n",
      "treated_nontarget_rpu: 4.09\n",
      "treated_nontarget_nipu: -1.12\n",
      "nontreated_nontarget_rpu: 3.42\n",
      "nontreated_nontarget_nipu: 7.92\n",
      "--- with 69.99854850080868% targeting, print cpits to treat users and create incrementality in users ---\n",
      "--> in targeted users: \n",
      "cpit = 13.71689025204352\n",
      "--> in non-targeted users: \n",
      "cpit = 13.377225287123306\n",
      "rpu_control: 3.42\n",
      "nipu_control: 7.67\n",
      "rpu_ft: 4.06\n",
      "nipu_ft: -1.03\n",
      "rpu_cohort: 3.86\n",
      "nipu_cohort: 1.69\n",
      "lift targeted cohort vs control: 0.13\n",
      "lift random vs control: 0.19\n",
      "cpit cohort vs control: 13.71\n",
      "lift targeted-treated vs control: 0.18\n",
      "cpit cohort: 13.714651\n",
      "---------------------------->>>>>>\n",
      "perc - target: 0.80\n",
      "treated_target_rpu: 4.05\n",
      "treated_target_nipu: -1.02\n",
      "nontreated_target_rpu: 3.42\n",
      "nontreated_target_nipu: 7.62\n",
      "treated_nontarget_rpu: 4.10\n",
      "treated_nontarget_nipu: -1.03\n",
      "nontreated_nontarget_rpu: 3.43\n",
      "nontreated_nontarget_nipu: 7.88\n",
      "--- with 79.99834114378136% targeting, print cpits to treat users and create incrementality in users ---\n",
      "--> in targeted users: \n",
      "cpit = 13.685923261954306\n",
      "--> in non-targeted users: \n",
      "cpit = 13.322579068783861\n",
      "rpu_control: 3.42\n",
      "nipu_control: 7.67\n",
      "rpu_ft: 4.06\n",
      "nipu_ft: -1.03\n",
      "rpu_cohort: 3.93\n",
      "nipu_cohort: 0.76\n",
      "lift targeted cohort vs control: 0.15\n",
      "lift random vs control: 0.19\n",
      "cpit cohort vs control: 13.68\n",
      "lift targeted-treated vs control: 0.18\n",
      "cpit cohort: 13.683241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------->>>>>>\n",
      "perc - target: 0.90\n",
      "treated_target_rpu: 4.06\n",
      "treated_target_nipu: -1.03\n",
      "nontreated_target_rpu: 3.44\n",
      "nontreated_target_nipu: 7.65\n",
      "treated_nontarget_rpu: 4.06\n",
      "treated_nontarget_nipu: -0.98\n",
      "nontreated_nontarget_rpu: 3.25\n",
      "nontreated_nontarget_nipu: 7.90\n",
      "--- with 89.99813378675402% targeting, print cpits to treat users and create incrementality in users ---\n",
      "--> in targeted users: \n",
      "cpit = 13.970549992635863\n",
      "--> in non-targeted users: \n",
      "cpit = 11.05455616377304\n",
      "rpu_control: 3.42\n",
      "nipu_control: 7.67\n",
      "rpu_ft: 4.06\n",
      "nipu_ft: -1.03\n",
      "rpu_cohort: 3.98\n",
      "nipu_cohort: -0.14\n",
      "lift targeted cohort vs control: 0.16\n",
      "lift random vs control: 0.19\n",
      "cpit cohort vs control: 13.97\n",
      "lift targeted-treated vs control: 0.19\n",
      "cpit cohort: 13.973435\n",
      "---------------------------->>>>>>\n",
      "perc - target: 1.00\n",
      "treated_target_rpu: 4.06\n",
      "treated_target_nipu: -1.03\n",
      "nontreated_target_rpu: 3.42\n",
      "nontreated_target_nipu: 7.67\n",
      "treated_nontarget_rpu: 2.33\n",
      "treated_nontarget_nipu: 4.23\n",
      "nontreated_nontarget_rpu: 0.00\n",
      "nontreated_nontarget_nipu: 0.00\n",
      "--- with 99.9979264297267% targeting, print cpits to treat users and create incrementality in users ---\n",
      "--> in targeted users: \n",
      "cpit = 13.608486021841472\n",
      "--> in non-targeted users: \n",
      "cpit = -1.8133636202142855\n",
      "rpu_control: 3.42\n",
      "nipu_control: 7.67\n",
      "rpu_ft: 4.06\n",
      "nipu_ft: -1.03\n",
      "rpu_cohort: 4.06\n",
      "nipu_cohort: -1.03\n",
      "lift targeted cohort vs control: 0.19\n",
      "lift random vs control: 0.19\n",
      "cpit cohort vs control: 13.61\n",
      "lift targeted-treated vs control: 0.19\n",
      "cpit cohort: 13.608044\n",
      "rpu_control: 3.4214800975660387\n",
      "nipu_control: 7.671282771491557\n",
      "rpu_ft: 4.0606804607232165\n",
      "nipu_ft: -1.02621382073425\n",
      "---------------------------->>>>>>\n",
      "perc - target: 0.10\n",
      "treated_target_rpu: 18.27\n",
      "treated_target_nipu: -24.86\n",
      "nontreated_target_rpu: 14.60\n",
      "nontreated_target_nipu: 24.03\n",
      "treated_nontarget_rpu: 2.48\n",
      "treated_nontarget_nipu: 1.62\n",
      "nontreated_nontarget_rpu: 2.18\n",
      "nontreated_nontarget_nipu: 5.85\n",
      "--- with 9.99979264297267% targeting, print cpits to treat users and create incrementality in users ---\n",
      "--> in targeted users: \n",
      "cpit = 13.322796401705451\n",
      "--> in non-targeted users: \n",
      "cpit = 13.802402305113354\n",
      "rpu_control: 3.42\n",
      "nipu_control: 7.67\n",
      "rpu_ft: 4.06\n",
      "nipu_ft: -1.03\n",
      "rpu_cohort: 3.79\n",
      "nipu_cohort: 2.78\n",
      "lift targeted cohort vs control: 0.11\n",
      "lift random vs control: 0.19\n",
      "cpit cohort vs control: 13.43\n",
      "lift targeted-treated vs control: 4.34\n",
      "cpit cohort: 13.431118\n",
      "---------------------------->>>>>>\n",
      "perc - target: 0.20\n",
      "treated_target_rpu: 12.67\n",
      "treated_target_nipu: -12.98\n",
      "nontreated_target_rpu: 10.31\n",
      "nontreated_target_nipu: 18.95\n",
      "treated_nontarget_rpu: 1.90\n",
      "treated_nontarget_nipu: 1.97\n",
      "nontreated_nontarget_rpu: 1.71\n",
      "nontreated_nontarget_nipu: 4.87\n",
      "--- with 19.99958528594534% targeting, print cpits to treat users and create incrementality in users ---\n",
      "--> in targeted users: \n",
      "cpit = 13.499659859095223\n",
      "--> in non-targeted users: \n",
      "cpit = 15.172083542989473\n",
      "rpu_control: 3.42\n",
      "nipu_control: 7.67\n",
      "rpu_ft: 4.06\n",
      "nipu_ft: -1.03\n",
      "rpu_cohort: 3.90\n",
      "nipu_cohort: 1.30\n",
      "lift targeted cohort vs control: 0.14\n",
      "lift random vs control: 0.19\n",
      "cpit cohort vs control: 13.19\n",
      "lift targeted-treated vs control: 2.70\n",
      "cpit cohort: 13.188917\n",
      "---------------------------->>>>>>\n",
      "perc - target: 0.30\n",
      "treated_target_rpu: 9.78\n",
      "treated_target_nipu: -8.23\n",
      "nontreated_target_rpu: 7.97\n",
      "nontreated_target_nipu: 15.26\n",
      "treated_nontarget_rpu: 1.61\n",
      "treated_nontarget_nipu: 2.06\n",
      "nontreated_nontarget_rpu: 1.47\n",
      "nontreated_nontarget_nipu: 4.41\n",
      "--- with 29.99937792891801% targeting, print cpits to treat users and create incrementality in users ---\n",
      "--> in targeted users: \n",
      "cpit = 12.988607231007224\n",
      "--> in non-targeted users: \n",
      "cpit = 16.680485897707264\n",
      "rpu_control: 3.42\n",
      "nipu_control: 7.67\n",
      "rpu_ft: 4.06\n",
      "nipu_ft: -1.03\n",
      "rpu_cohort: 3.96\n",
      "nipu_cohort: 0.62\n",
      "lift targeted cohort vs control: 0.16\n",
      "lift random vs control: 0.19\n",
      "cpit cohort vs control: 13.03\n",
      "lift targeted-treated vs control: 1.86\n",
      "cpit cohort: 13.033568\n",
      "---------------------------->>>>>>\n",
      "perc - target: 0.40\n",
      "treated_target_rpu: 7.97\n",
      "treated_target_nipu: -5.82\n",
      "nontreated_target_rpu: 6.49\n",
      "nontreated_target_nipu: 12.77\n",
      "treated_nontarget_rpu: 1.46\n",
      "treated_nontarget_nipu: 2.17\n",
      "nontreated_nontarget_rpu: 1.37\n",
      "nontreated_nontarget_nipu: 4.26\n",
      "--- with 39.99917057189068% targeting, print cpits to treat users and create incrementality in users ---\n",
      "--> in targeted users: \n",
      "cpit = 12.640373591218138\n",
      "--> in non-targeted users: \n",
      "cpit = 22.883738179299165\n",
      "rpu_control: 3.42\n",
      "nipu_control: 7.67\n",
      "rpu_ft: 4.06\n",
      "nipu_ft: -1.03\n",
      "rpu_cohort: 4.01\n",
      "nipu_cohort: 0.23\n",
      "lift targeted cohort vs control: 0.17\n",
      "lift random vs control: 0.19\n",
      "cpit cohort vs control: 12.72\n",
      "lift targeted-treated vs control: 1.33\n",
      "cpit cohort: 12.715531\n",
      "---------------------------->>>>>>\n",
      "perc - target: 0.50\n",
      "treated_target_rpu: 6.75\n",
      "treated_target_nipu: -4.29\n",
      "nontreated_target_rpu: 5.55\n",
      "nontreated_target_nipu: 11.18\n",
      "treated_nontarget_rpu: 1.37\n",
      "treated_nontarget_nipu: 2.24\n",
      "nontreated_nontarget_rpu: 1.30\n",
      "nontreated_nontarget_nipu: 4.17\n",
      "--- with 49.99896321486335% targeting, print cpits to treat users and create incrementality in users ---\n",
      "--> in targeted users: \n",
      "cpit = 12.94008576990308\n",
      "--> in non-targeted users: \n",
      "cpit = 25.813153881274527\n",
      "rpu_control: 3.42\n",
      "nipu_control: 7.67\n",
      "rpu_ft: 4.06\n",
      "nipu_ft: -1.03\n",
      "rpu_cohort: 4.02\n",
      "nipu_cohort: -0.06\n",
      "lift targeted cohort vs control: 0.18\n",
      "lift random vs control: 0.19\n",
      "cpit cohort vs control: 12.87\n",
      "lift targeted-treated vs control: 0.97\n",
      "cpit cohort: 12.866048\n",
      "---------------------------->>>>>>\n",
      "perc - target: 0.60\n",
      "treated_target_rpu: 5.89\n",
      "treated_target_nipu: -3.27\n",
      "nontreated_target_rpu: 4.87\n",
      "nontreated_target_nipu: 9.96\n",
      "treated_nontarget_rpu: 1.31\n",
      "treated_nontarget_nipu: 2.34\n",
      "nontreated_nontarget_rpu: 1.25\n",
      "nontreated_nontarget_nipu: 4.23\n",
      "--- with 59.99875585783602% targeting, print cpits to treat users and create incrementality in users ---\n",
      "--> in targeted users: \n",
      "cpit = 12.878970153638202\n",
      "--> in non-targeted users: \n",
      "cpit = 30.431899999223393\n",
      "rpu_control: 3.42\n",
      "nipu_control: 7.67\n",
      "rpu_ft: 4.06\n",
      "nipu_ft: -1.03\n",
      "rpu_cohort: 4.04\n",
      "nipu_cohort: -0.27\n",
      "lift targeted cohort vs control: 0.18\n",
      "lift random vs control: 0.19\n",
      "cpit cohort vs control: 12.92\n",
      "lift targeted-treated vs control: 0.72\n",
      "cpit cohort: 12.915797\n",
      "---------------------------->>>>>>\n",
      "perc - target: 0.70\n",
      "treated_target_rpu: 5.26\n",
      "treated_target_nipu: -2.55\n",
      "nontreated_target_rpu: 4.36\n",
      "nontreated_target_nipu: 9.11\n",
      "treated_nontarget_rpu: 1.26\n",
      "treated_nontarget_nipu: 2.53\n",
      "nontreated_nontarget_rpu: 1.24\n",
      "nontreated_nontarget_nipu: 4.34\n",
      "--- with 69.99854850080868% targeting, print cpits to treat users and create incrementality in users ---\n",
      "--> in targeted users: \n",
      "cpit = 13.045072184912907\n",
      "--> in non-targeted users: \n",
      "cpit = 64.40135913731166\n",
      "rpu_control: 3.42\n",
      "nipu_control: 7.67\n",
      "rpu_ft: 4.06\n",
      "nipu_ft: -1.03\n",
      "rpu_cohort: 4.05\n",
      "nipu_cohort: -0.48\n",
      "lift targeted cohort vs control: 0.18\n",
      "lift random vs control: 0.19\n",
      "cpit cohort vs control: 12.95\n",
      "lift targeted-treated vs control: 0.54\n",
      "cpit cohort: 12.949179\n",
      "---------------------------->>>>>>\n",
      "perc - target: 0.80\n",
      "treated_target_rpu: 4.77\n",
      "treated_target_nipu: -1.97\n",
      "nontreated_target_rpu: 3.98\n",
      "nontreated_target_nipu: 8.47\n",
      "treated_nontarget_rpu: 1.23\n",
      "treated_nontarget_nipu: 2.77\n",
      "nontreated_nontarget_rpu: 1.20\n",
      "nontreated_nontarget_nipu: 4.48\n",
      "--- with 79.99834114378136% targeting, print cpits to treat users and create incrementality in users ---\n",
      "--> in targeted users: \n",
      "cpit = 13.238747359646212\n",
      "--> in non-targeted users: \n",
      "cpit = 58.57004900683186\n",
      "rpu_control: 3.42\n",
      "nipu_control: 7.67\n",
      "rpu_ft: 4.06\n",
      "nipu_ft: -1.03\n",
      "rpu_cohort: 4.05\n",
      "nipu_cohort: -0.68\n",
      "lift targeted cohort vs control: 0.18\n",
      "lift random vs control: 0.19\n",
      "cpit cohort vs control: 13.20\n",
      "lift targeted-treated vs control: 0.39\n",
      "cpit cohort: 13.202359\n",
      "---------------------------->>>>>>\n",
      "perc - target: 0.90\n",
      "treated_target_rpu: 4.37\n",
      "treated_target_nipu: -1.53\n",
      "nontreated_target_rpu: 3.68\n",
      "nontreated_target_nipu: 7.97\n",
      "treated_nontarget_rpu: 1.23\n",
      "treated_nontarget_nipu: 3.50\n",
      "nontreated_nontarget_rpu: 1.18\n",
      "nontreated_nontarget_nipu: 5.05\n",
      "--- with 89.99813378675402% targeting, print cpits to treat users and create incrementality in users ---\n",
      "--> in targeted users: \n",
      "cpit = 13.661101888012919\n",
      "--> in non-targeted users: \n",
      "cpit = 29.81832280118425\n",
      "rpu_control: 3.42\n",
      "nipu_control: 7.67\n",
      "rpu_ft: 4.06\n",
      "nipu_ft: -1.03\n",
      "rpu_cohort: 4.05\n",
      "nipu_cohort: -0.87\n",
      "lift targeted cohort vs control: 0.18\n",
      "lift random vs control: 0.19\n",
      "cpit cohort vs control: 13.51\n",
      "lift targeted-treated vs control: 0.28\n",
      "cpit cohort: 13.511330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../experimentation.py:112: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "  untreated_untarget_rpu = sum(1.0 * values[untreated_untargeted_filter]) / sum(untreated_untargeted_filter)\n",
      "../experimentation.py:113: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "  untreated_untarget_nipu = sum(1.0 * n9d_ni_usd[untreated_untargeted_filter]) / sum(untreated_untargeted_filter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------->>>>>>\n",
      "perc - target: 1.00\n",
      "treated_target_rpu: 4.06\n",
      "treated_target_nipu: -1.03\n",
      "nontreated_target_rpu: 3.42\n",
      "nontreated_target_nipu: 7.67\n",
      "treated_nontarget_rpu: 10.50\n",
      "treated_nontarget_nipu: 71.74\n",
      "nontreated_nontarget_rpu: 0.00\n",
      "nontreated_nontarget_nipu: 0.00\n",
      "--- with 99.9979264297267% targeting, print cpits to treat users and create incrementality in users ---\n",
      "--> in targeted users: \n",
      "cpit = 13.61334293582091\n",
      "--> in non-targeted users: \n",
      "cpit = -6.832367125017619\n",
      "rpu_control: 3.42\n",
      "nipu_control: 7.67\n",
      "rpu_ft: 4.06\n",
      "nipu_ft: -1.03\n",
      "rpu_cohort: 4.06\n",
      "nipu_cohort: -1.03\n",
      "lift targeted cohort vs control: 0.19\n",
      "lift random vs control: 0.19\n",
      "cpit cohort vs control: 13.62\n",
      "lift targeted-treated vs control: 0.19\n",
      "cpit cohort: 13.615103\n",
      "rpu_control: 3.4214800975660387\n",
      "nipu_control: 7.671282771491557\n",
      "rpu_ft: 4.0606804607232165\n",
      "nipu_ft: -1.02621382073425\n",
      "---------------------------->>>>>>\n",
      "perc - target: 0.10\n",
      "treated_target_rpu: 2.45\n",
      "treated_target_nipu: -1.04\n",
      "nontreated_target_rpu: 1.81\n",
      "nontreated_target_nipu: 2.74\n",
      "treated_nontarget_rpu: 4.24\n",
      "treated_nontarget_nipu: -1.02\n",
      "nontreated_nontarget_rpu: 3.60\n",
      "nontreated_nontarget_nipu: 8.21\n",
      "--- with 9.99979264297267% targeting, print cpits to treat users and create incrementality in users ---\n",
      "--> in targeted users: \n",
      "cpit = 5.919963125096151\n",
      "--> in non-targeted users: \n",
      "cpit = 14.404952203203207\n",
      "rpu_control: 3.42\n",
      "nipu_control: 7.67\n",
      "rpu_ft: 4.06\n",
      "nipu_ft: -1.03\n",
      "rpu_cohort: 3.48\n",
      "nipu_cohort: 7.29\n",
      "lift targeted cohort vs control: 0.02\n",
      "lift random vs control: 0.19\n",
      "cpit cohort vs control: 6.13\n",
      "lift targeted-treated vs control: -0.29\n",
      "cpit cohort: 6.132458\n",
      "---------------------------->>>>>>\n",
      "perc - target: 0.20\n",
      "treated_target_rpu: 2.27\n",
      "treated_target_nipu: -0.54\n",
      "nontreated_target_rpu: 1.78\n",
      "nontreated_target_nipu: 3.01\n",
      "treated_nontarget_rpu: 4.51\n",
      "treated_nontarget_nipu: -1.15\n",
      "nontreated_nontarget_rpu: 3.83\n",
      "nontreated_nontarget_nipu: 8.83\n",
      "--- with 19.99958528594534% targeting, print cpits to treat users and create incrementality in users ---\n",
      "--> in targeted users: \n",
      "cpit = 7.23750503575353\n",
      "--> in non-targeted users: \n",
      "cpit = 14.688843727835557\n",
      "rpu_control: 3.42\n",
      "nipu_control: 7.67\n",
      "rpu_ft: 4.06\n",
      "nipu_ft: -1.03\n",
      "rpu_cohort: 3.52\n",
      "nipu_cohort: 6.96\n",
      "lift targeted cohort vs control: 0.03\n",
      "lift random vs control: 0.19\n",
      "cpit cohort vs control: 7.43\n",
      "lift targeted-treated vs control: -0.34\n",
      "cpit cohort: 7.428944\n"
     ]
    }
   ],
   "source": [
    "### code implements ranking model for treatment effect \n",
    "### for optimizing with respect to direct marketplace objectives \n",
    "### using tensorflow \n",
    "\n",
    "import numpy as np, tensorflow as tf, pandas as pd, pickle as pkl \n",
    "sys.path.append('../')  \n",
    "from ModelDefinitions import * \n",
    "from DataProcFunctions import * \n",
    "\n",
    "### RxGy TQR setting: \n",
    "p_quantile = 0.4 ## percentage of quantile to aim for \n",
    "num_optimize_iterations = 100 ## number of optimization iterations \n",
    "num_modeling_inits = 2 ## number of random initializations \n",
    "num_hidden = 0 ## number of hidden units in DNN \n",
    "use_schedule = True ## option to use a constraint annealing schedule \n",
    "temp = 0.5 ## initial temperature for constraints \n",
    "inc_temp = 0.1 ## increment of temperature per 100 iterations \n",
    "save_cf_data = False ### whether to save data for causal forest training \n",
    "\n",
    "## set a random seed to reproduce results \n",
    "seed = 1234; tf.compat.v2.random.set_seed(seed); np.random.seed(seed) \n",
    "\n",
    "sample_frac = 1.0 ## option to sample data by a fraction \\in (0, 1) \n",
    "data_filename =  '../data/rxgy_ma_training_data_v5_2019_07_08_vc_tr_featuremod3' \n",
    "prefix = 'rxgy_v5_07_08_featuremod3_tr_iter100_run4' \n",
    "\n",
    "D_tre, D_unt, Dv_tre, Dv_unt, Dt_tre, Dt_unt, o_tre, o_unt, ov_tre, ov_unt, ot_tre, ot_unt, c_tre, c_unt, cv_tre, cv_unt, ct_tre, ct_unt, D, w, o, c, Dv, wv, ov, cv, Dt, wt, ot, ct = LoadDataFromPkl(data_filename, frac = sample_frac, save_cf_data=save_cf_data) \n",
    "\n",
    "print('### ----- start the training of deep learning models ------ ') \n",
    "gs_tqr = [] \n",
    "gs_drm = [] \n",
    "for i in range(num_modeling_inits): \n",
    "    gs_tqr.append(tf.Graph()) \n",
    "for i in range(num_modeling_inits): \n",
    "    gs_drm.append(tf.Graph()) \n",
    "\n",
    "print('------> Training TQR ranking model .... ') \n",
    "val_results = [] \n",
    "sess_list = [] \n",
    "for i in range(num_modeling_inits): \n",
    "    print('---> running cross validation, iteration: ' + str(i)) \n",
    "    obj, opt, dumh, dumhu, vtemp, p_quantile = TunableTQRankingModelDNN(gs_tqr[i], D_tre, D_unt, o_tre, o_unt, c_tre, c_unt, 'train-first', temp, p_quantile, num_hidden, use_schedule) \n",
    "    ### session definitions and variable initialization \n",
    "    sess = tf.Session(graph = gs_tqr[i]) \n",
    "    sess_list.append(sess) \n",
    "    \n",
    "    ### initialize variables and run optimization \n",
    "    with gs_tqr[i].as_default() as g: \n",
    "        init = tf.global_variables_initializer() \n",
    "    sess.run(init) \n",
    "    cur_temp = temp \n",
    "    for step in range(num_optimize_iterations): \n",
    "        _, objres = sess.run([opt, obj]) \n",
    "        if step % 100 == 0: \n",
    "            cur_temp = cur_temp + inc_temp \n",
    "            print('opt. step : ' + str(step) + ' obj: ' + str(objres)) \n",
    "            if use_schedule: \n",
    "                sess.run(vtemp.assign(cur_temp))\n",
    "                print('setting temperature to :' + str(sess.run(vtemp))) \n",
    "    \n",
    "    print('---> optimization finished ... ') \n",
    "    tempvalue = sess.run(vtemp)\n",
    "    p_quantilevalue = p_quantile\n",
    "    print('temp:') \n",
    "    print(tempvalue)\n",
    "    print('p_quantile:')\n",
    "    print(p_quantilevalue) \n",
    "    \n",
    "    ### evaluate CPIT metric on validation set \n",
    "    objv, dumo, dumh, dumhu, dvtemp, dp_quantile = TunableTQRankingModelDNN(gs_tqr[i], Dv_tre, Dv_unt, ov_tre, ov_unt, cv_tre, cv_unt, 'eval', temp, p_quantile, num_hidden, use_schedule) \n",
    "    \n",
    "    val_result = sess.run(objv) \n",
    "    print('validation CPIT:') \n",
    "    print(val_result) \n",
    "    val_results.append(val_result) \n",
    "\n",
    "from operator import itemgetter \n",
    "best_index = min(enumerate(val_results), key=itemgetter(1))[0] \n",
    "\n",
    "print('best performing model: iteration ' + str(best_index)) \n",
    "\n",
    "### run scoring on whole test set \n",
    "with gs_tqr[best_index].as_default() as g: \n",
    "    if num_hidden > 0: \n",
    "        with tf.variable_scope(\"tqrhidden\") as scope: \n",
    "            h1_test = tf.contrib.layers.fully_connected(Dt, num_hidden, activation_fn=tf.nn.tanh, reuse=tf.AUTO_REUSE, scope=scope, weights_initializer=tf.contrib.layers.xavier_initializer()) \n",
    "        with tf.variable_scope(\"tqranker\") as scope: \n",
    "            h_test = tf.contrib.layers.fully_connected(h1_test, 1, activation_fn=None, reuse=tf.AUTO_REUSE, scope=scope) \n",
    "    else: \n",
    "        with tf.variable_scope(\"tqranker\") as scope: \n",
    "            h_test = tf.contrib.layers.fully_connected(Dt, 1, activation_fn=None, reuse=tf.AUTO_REUSE, scope=scope) \n",
    "    tqrscore = sess_list[best_index].run(h_test) \n",
    "\n",
    "print('------> Training DRM ranking model .... ') \n",
    "sess_list = [] \n",
    "val_results = [] \n",
    "for i in range(num_modeling_inits): \n",
    "    print('---> running cross validation, iteration: ' + str(i)) \n",
    "    ### ---- train cpit ranking model for comparison --- \n",
    "    dobjc, doptc, ddumh, ddumu = DirectRankingModelDNN(gs_drm[i], D_tre, D_unt, o_tre, o_unt, c_tre, c_unt, 'train-first-drm', num_hidden) \n",
    "    \n",
    "    dsess = tf.Session(graph = gs_drm[i]) \n",
    "    sess_list.append(dsess) \n",
    "    \n",
    "    ### initialize variables and run optimization \n",
    "    with gs_drm[i].as_default() as g: \n",
    "        dinit = tf.global_variables_initializer() \n",
    "    dsess.run(dinit) \n",
    "    for step in range(num_optimize_iterations): \n",
    "        _, dobjres = dsess.run([doptc, dobjc]) \n",
    "        if step % 100 == 0: \n",
    "            print('opt. step : ' + str(step) + ' obj: ' + str(dobjres)) \n",
    "    \n",
    "    print('---> optimization finished ... ') \n",
    "    \n",
    "    ### evaluate CPIT metric on validation set \n",
    "    dobjv, ddumo, dumh, dumhu = DirectRankingModelDNN(gs_drm[i], Dv_tre, Dv_unt, ov_tre, ov_unt, cv_tre, cv_unt, 'eval', num_hidden)\n",
    "    val_result = dsess.run(dobjv) \n",
    "    print('validation CPIT:') \n",
    "    print(val_result) \n",
    "    val_results.append(val_result) \n",
    "\n",
    "best_index = min(enumerate(val_results), key=itemgetter(1))[0] \n",
    "\n",
    "print('best performing model: iteration ' + str(best_index)) \n",
    "\n",
    "### run scoring on whole test set \n",
    "with gs_drm[best_index].as_default() as g: \n",
    "    if num_hidden > 0: \n",
    "        with tf.variable_scope(\"drmhidden\") as scope: \n",
    "            h1_test = tf.contrib.layers.fully_connected(Dt, num_hidden, activation_fn=tf.nn.tanh, reuse=tf.AUTO_REUSE, scope=scope, weights_initializer=tf.contrib.layers.xavier_initializer()) \n",
    "        with tf.variable_scope(\"drmranker\") as scope: \n",
    "            h_test = tf.contrib.layers.fully_connected(h1_test, 1, activation_fn=tf.nn.tanh, reuse=tf.AUTO_REUSE, scope=scope) \n",
    "    else: \n",
    "        with tf.variable_scope(\"drmranker\") as scope: \n",
    "            h_test = tf.contrib.layers.fully_connected(Dt, 1, activation_fn=tf.nn.tanh, reuse=tf.AUTO_REUSE, scope=scope) \n",
    "    drmscore = sess_list[best_index].run(h_test) \n",
    "\n",
    "### ---- train hte model for comparison ---- \n",
    "### we could utimize the original HTE functions \n",
    "from LinearHTEModels import * \n",
    "from PromotionModels import PromotionModels \n",
    "\n",
    "pmodels = PromotionModels() \n",
    "\n",
    "## set-up RLearner \n",
    "rl_ridge_model_O, rl_ridge_model_C = pmodels.fit_rlearner(D, o, c, w) \n",
    "\n",
    "## one model for order lift and one model for cost drop \n",
    "pred_values_va_rlearner_O = rl_ridge_model_O.predict(Dt) \n",
    "pred_values_va_rlearner_C = rl_ridge_model_C.predict(Dt) \n",
    "\n",
    "plt.imagesc(np.concatneate(rl_ridge_model_O.params, ...params), axis=1) \n",
    "\n",
    "#if ranking_model == 'effectiveness-ratio': ## if we use the effectiveness ratio model, compute effectiveness ratio \n",
    "pred_values_va_rlearner = np.divide(np.maximum(pred_values_va_rlearner_O, 0), pred_values_va_rlearner_C + 1e-7) \n",
    "\n",
    "lhmodels = LinearHTEModels() \n",
    "lambds = [0.1] \n",
    "rlearnerscores = [] \n",
    "rl_ridge_model_L_list = [] \n",
    "## set-up lagrangian rlearner \n",
    "for i in range(len(lambds)): \n",
    "    lambd = lambds[i] \n",
    "    rl_ridge_model_L = lhmodels.fit_rlearner_lagrangian(D, o, c, w, lambd) \n",
    "    rl_ridge_model_L_list.append(rl_ridge_model_L) \n",
    "    rlearnerscores.append(rl_ridge_model_L.predict(Dt)) \n",
    "\n",
    "\"\"\" \n",
    "### this section is to load the results trained by grf R code \n",
    "### \n",
    "\n",
    "ot_cf = pd.read_csv('../results/causal_forest_grf_test_set_results_O_finalsize_numtrees1002.csv') \n",
    "ct_cf = pd.read_csv('../results/causal_forest_grf_test_set_results_C_finalsize_numtrees1002.csv') \n",
    "\n",
    "ot_cf = ot_cf.as_matrix() \n",
    "Ocfscores = ot_cf[0] \n",
    "\n",
    "ct_cf = ct_cf.as_matrix() \n",
    "Ccfscores = ct_cf[0] \n",
    "\n",
    "cfscore = np.divide(Ocfscores, Ccfscores) \n",
    "\"\"\" \n",
    "\n",
    "### ---- experimentation and plotting cost-curves ----- \n",
    "from experimentation import * \n",
    "exp = Experimentation() \n",
    "ranscore = np.random.rand(ot.shape[0], ) \n",
    "colors = ['b', 'c', 'g', 'y'] \n",
    "plt.figure() \n",
    "rlearnerauccs = [] \n",
    "ranaucc = exp.AUC_cpit_cost_curve_deciles_cohort(ranscore, ot, wt, -1.0 * ct, 'k', plot_random=True) \n",
    "quasiaucc = exp.AUC_cpit_cost_curve_deciles_cohort(pred_values_va_rlearner_O, ot, wt, -1.0 * ct, 'c') \n",
    "for i in range(len(lambds)): \n",
    "    rlearnerauccs.append(exp.AUC_cpit_cost_curve_deciles_cohort(rlearnerscores[i], ot, wt, -1.0 * ct, colors[i] )) \n",
    "#cfaucc = exp.AUC_cpit_cost_curve_deciles_cohort(cfscore, ot, wt, -1.0 * ct, 'g') # causal forest aucc and plotting \n",
    "tqraucc = exp.AUC_cpit_cost_curve_deciles_cohort(tqrscore, ot, wt, -1.0 * ct, 'r' ) \n",
    "drmaucc = exp.AUC_cpit_cost_curve_deciles_cohort(drmscore, ot, wt, -1.0 * ct, 'm' ) \n",
    "plt.title('Causal learning cost curves using targeting models') \n",
    "\n",
    "print('temp:') \n",
    "print(tempvalue) \n",
    "print('p_quantile:') \n",
    "print(p_quantilevalue) \n",
    "\n",
    "### --- add legeneds to plot ---- \n",
    "leg_str = ['Random'] \n",
    "leg_str.append('R-learner on Incremental Gain') \n",
    "for i in range(len(lambds)): \n",
    "    leg_str.append('Duality R-learner') \n",
    "leg_str.append('Causal Forest') # causal forest result \n",
    "leg_str.append('Top Quantile Ranking at ' + str(p_quantile*100) + '%') \n",
    "leg_str.append('Direct Ranking Model') \n",
    "plt.legend(leg_str) \n",
    "\n",
    "### --- print out aucc results for different models --- \n",
    "print('AUCC results: ') \n",
    "print('random: ' + str(ranaucc)) \n",
    "print('rlearner: ' + str(quasiaucc)) \n",
    "i = 0\n",
    "for rlearneraucc in rlearnerauccs: \n",
    "    print('duality rlearner ' + str(i + 1) + ' with lambda = ' + str(lambds[i]) + ':' + str(rlearneraucc)) \n",
    "    i = i + 1\n",
    "print('drm: ' + str(drmaucc)) \n",
    "print('tqr: ' + str(tqraucc)) \n",
    "\n",
    "plt.show() \n",
    "\n",
    "### --- saving data to results folder ---- \n",
    "save_filename = '../results/benchmarkwithcv_tqr_drm_hte_'+prefix+'_main_results.pkl' \n",
    "saveD = {'tqrscore':tqrscore, 'drmscore':drmscore, 'quasiscore':pred_values_va_rlearner, 'quasiscore_O':pred_values_va_rlearner_O, 'rlearnerscore':rlearnerscores, 'ot':ot, 'wt':wt, 'ct':ct, 'tempvalue':tempvalue, 'p_quantilevalue':p_quantilevalue, \n",
    "         'tqraucc':tqraucc, 'drmaucc':drmaucc, 'rlearnerauccs':rlearnerauccs, 'ranaucc':ranaucc, 'quasiaucc':quasiaucc} \n",
    "#'cfscore':cfscore, causal forest scores \n",
    "pkl.dump(saveD, open(save_filename, 'w')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MLVis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
